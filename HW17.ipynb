{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uit0QoxLhdVH",
        "outputId": "c47507c1-49a2-4956-b402-3a3a5521995c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting karateclub==1.2.0\n",
            "  Downloading karateclub-1.2.0.tar.gz (59 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/59.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from karateclub==1.2.0) (1.26.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from karateclub==1.2.0) (3.3)\n",
            "Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.10/dist-packages (from karateclub==1.2.0) (4.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from karateclub==1.2.0) (4.66.5)\n",
            "Requirement already satisfied: python-louvain in /usr/local/lib/python3.10/dist-packages (from karateclub==1.2.0) (0.16)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from karateclub==1.2.0) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from karateclub==1.2.0) (1.13.1)\n",
            "Collecting pygsp (from karateclub==1.2.0)\n",
            "  Downloading PyGSP-0.5.1-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: gensim>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from karateclub==1.2.0) (4.3.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from karateclub==1.2.0) (2.1.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from karateclub==1.2.0) (1.16.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.0.0->karateclub==1.2.0) (7.0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->karateclub==1.2.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->karateclub==1.2.0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->karateclub==1.2.0) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->karateclub==1.2.0) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->karateclub==1.2.0) (3.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim>=4.0.0->karateclub==1.2.0) (1.16.0)\n",
            "Downloading PyGSP-0.5.1-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: karateclub\n",
            "  Building wheel for karateclub (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for karateclub: filename=karateclub-1.2.0-py3-none-any.whl size=94311 sha256=8466bf71b093512cb685e113dfbe826aa3ff39b818ca47a7fa702e55120bedb3\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/83/22/2ea49cf105f1a344e9f5813bc307112147af91b8480be84361\n",
            "Successfully built karateclub\n",
            "Installing collected packages: pygsp, karateclub\n",
            "Successfully installed karateclub-1.2.0 pygsp-0.5.1\n",
            "Collecting umap-learn\n",
            "  Downloading umap_learn-0.5.6-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.5.2)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.60.0)\n",
            "Collecting pynndescent>=0.5 (from umap-learn)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.66.5)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.5.0)\n",
            "Downloading umap_learn-0.5.6-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.13 umap-learn-0.5.6\n"
          ]
        }
      ],
      "source": [
        "!pip install karateclub==1.2.0\n",
        "!pip install umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id \"1RmrHId0d-uY7kJCSgCtNYbwYfp4Oum3c&export=download\"\n",
        "!unrar x -Y \"/content/lab3.rar\" -d \"/content/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n32-IVhJjTtR",
        "outputId": "c62d5605-5580-48f1-e293-b199e02f7884"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1RmrHId0d-uY7kJCSgCtNYbwYfp4Oum3c&export=download\n",
            "To: /content/lab3.rar\n",
            "100% 1.54M/1.54M [00:00<00:00, 73.7MB/s]\n",
            "\n",
            "UNRAR 6.11 beta 1 freeware      Copyright (c) 1993-2022 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from /content/lab3.rar\n",
            "\n",
            "Extracting  /content/lab3_attributes.csv                                 \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Extracting  /content/facebook_features.json                              \b\b\b\b 28%\b\b\b\b\b  OK \n",
            "Extracting  /content/facebook_target.csv                                 \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Extracting  /content/lab3_edgelist.txt                                   \b\b\b\b 59%\b\b\b\b\b  OK \n",
            "Extracting  /content/facebook_edges.csv                                  \b\b\b\b100%\b\b\b\b\b  OK \n",
            "All OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1\n",
        "import networkx as nx\n",
        "from joblib import Parallel, delayed\n",
        "import random\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Task 2\n",
        "import json\n",
        "import umap\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "from karateclub.utils.walker import RandomWalker\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "rG2CeOkijaB0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def partition_num(num, workers):\n",
        "    if num % workers == 0:\n",
        "        return [num//workers]*workers\n",
        "    else:\n",
        "        return [num//workers]*workers + [num % workers]\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()\n",
        "\n",
        "\n",
        "def get_attributes_of_node(node_paths):\n",
        "  node_paths_attributes = []\n",
        "  # Get attribute (word) for each node\n",
        "  df_attr = pd.read_csv(\"lab3_attributes.csv\").astype(str)\n",
        "  dict_attr = {}\n",
        "  for i in range(len(df_attr)):\n",
        "    dict_attr[df_attr.iloc[i, 0]] = df_attr.iloc[i, 1]\n",
        "  for path in node_paths:\n",
        "    for index, node in enumerate(path):\n",
        "      path[index] = dict_attr[node]\n",
        "    node_paths_attributes.append(path)\n",
        "  return node_paths_attributes\n",
        "\n",
        "def preprocessing(sentences):\n",
        "    training_data = []\n",
        "    for sentence in sentences:\n",
        "        x = [word for word in sentence]\n",
        "        training_data.append(x)\n",
        "    return training_data\n",
        "\n",
        "\n",
        "def prepare_data_for_training(sentences,w2v):\n",
        "    data = {}\n",
        "    for sentence in sentences:\n",
        "        for word in sentence:\n",
        "            if word not in data:\n",
        "                data[word] = 1\n",
        "            else:\n",
        "                data[word] += 1\n",
        "    V = len(data)\n",
        "    data = sorted(list(data.keys()))\n",
        "    vocab = {}\n",
        "    for i in range(len(data)):\n",
        "        vocab[data[i]] = i\n",
        "\n",
        "    for sentence in sentences:\n",
        "        for i in range(len(sentence)):\n",
        "            center_word = [0 for x in range(V)]\n",
        "            center_word[vocab[sentence[i]]] = 1\n",
        "            context = [0 for x in range(V)]\n",
        "\n",
        "            for j in range(i-w2v.window_size,i+w2v.window_size):\n",
        "                if i!=j and j>=0 and j<len(sentence):\n",
        "                    context[vocab[sentence[j]]] += 1\n",
        "            w2v.X_train.append(center_word)\n",
        "            w2v.y_train.append(context)\n",
        "    w2v.initialize(V,data)\n",
        "\n",
        "    return w2v.X_train,w2v.y_train"
      ],
      "metadata": {
        "id": "-ZFivy09eFpj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class word2vec():\n",
        "    def __init__(self, window_size=2, n_hidden=10, learning_rate=0.001, epoch=300):\n",
        "        self.window_size = window_size  # Size of the context window\n",
        "        self.n_hidden = n_hidden        # Number of hidden neurons\n",
        "        self.learning_rate = learning_rate  # Learning rate for SGD\n",
        "        self.X_train = []               # Input vectors (one-hot encoded)\n",
        "        self.y_train = []               # Target context vectors (one-hot encoded)\n",
        "        self.V = None                   # Vocabulary size\n",
        "        self.vocab = None               # Vocabulary list\n",
        "        self.W1 = None                 # Weights between input and hidden layer\n",
        "        self.W2 = None                  # Weights between hidden and output layer\n",
        "        self.epoch = epoch\n",
        "    def initialize(self, V, vocab):\n",
        "        \"\"\"\n",
        "        Initialize the weights and other parameters.\n",
        "        V: vocabulary size\n",
        "        vocab: list of unique words\n",
        "        \"\"\"\n",
        "        self.V = V\n",
        "        self.vocab = vocab\n",
        "        # Weight matrices initialization with small random numbers\n",
        "        self.W1 = np.random.uniform(-1, 1, (self.V, self.n_hidden))  # V x n_hidden\n",
        "        self.W2 = np.random.uniform(-1, 1, (self.n_hidden, self.V))  # n_hidden x V\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass: computes the hidden layer output and the output layer (predicted context).\n",
        "        x: input vector (one-hot encoded word)\n",
        "        \"\"\"\n",
        "        h = np.dot(x, self.W1)  # h is the hidden layer output\n",
        "        u = np.dot(h, self.W2)  # u is the output before applying softmax\n",
        "        y_pred = self.softmax(u)  # y_pred is the predicted context distribution\n",
        "        return h, y_pred\n",
        "\n",
        "    def softmax(self, x):\n",
        "        \"\"\"Compute softmax values for each set of scores in x.\"\"\"\n",
        "        e_x = np.exp(x - np.max(x))  # numerical stability\n",
        "        return e_x / np.sum(e_x)\n",
        "\n",
        "    def backpropagate(self, x, h, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Backpropagation: update weights based on prediction error.\n",
        "        x: input vector (one-hot encoded word)\n",
        "        h: hidden layer output\n",
        "        y_pred: predicted context (softmax output)\n",
        "        y_true: true context (one-hot encoded)\n",
        "        \"\"\"\n",
        "        # # Error at output layer\n",
        "        # error = y_pred - y_true\n",
        "\n",
        "        # # Gradient with respect to W2\n",
        "        # dW2 = np.outer(h, error)  # n_hidden x V\n",
        "\n",
        "        # # Gradient with respect to W1\n",
        "        # dh = np.dot(self.W2, error)  # Gradient from output to hidden\n",
        "        # dW1 = np.outer(x, dh)  # V x n_hidden\n",
        "\n",
        "        # # Update weights\n",
        "        # self.W1 -= self.learning_rate * dW1\n",
        "        # self.W2 -= self.learning_rate * dW2\n",
        "        error = y_pred - y_true\n",
        "        dW2 = np.outer(h, error)\n",
        "        dh = np.dot(self.W2, error)\n",
        "        dW1 = np.outer(x, dh)\n",
        "\n",
        "        # Gradient clipping to prevent exploding gradients\n",
        "        max_grad = 5.0\n",
        "        dW1 = np.clip(dW1, -max_grad, max_grad)\n",
        "        dW2 = np.clip(dW2, -max_grad, max_grad)\n",
        "\n",
        "        self.W1 -= self.learning_rate * dW1\n",
        "        self.W2 -= self.learning_rate * dW2\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Train the model using the training data (X_train and y_train).\n",
        "        epochs: number of iterations over the dataset\n",
        "        \"\"\"\n",
        "        for epoch in range(self.epoch):\n",
        "            loss = 0\n",
        "            for x, y_true in zip(self.X_train, self.y_train):\n",
        "                # Forward pass\n",
        "                h, y_pred = self.forward(np.array(x))\n",
        "\n",
        "                # Compute loss (cross-entropy)\n",
        "                loss += -np.sum(y_true * np.log(y_pred) )\n",
        "\n",
        "                # Backpropagation\n",
        "                self.backpropagate(np.array(x), h, y_pred, np.array(y_true))\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "    def get_word_vector(self, word):\n",
        "        \"\"\"\n",
        "        Get the word vector for a given word from the W1 matrix.\n",
        "        word: the word in the vocabulary\n",
        "        \"\"\"\n",
        "        word_index = self.vocab.index(word)\n",
        "        return self.W1[word_index]\n",
        "    def predict(self, word, top_n=3):\n",
        "        \"\"\"\n",
        "        Find the top N most similar words based on cosine similarity.\n",
        "        word: the word in the vocabulary\n",
        "        top_n: number of similar words to return\n",
        "        \"\"\"\n",
        "        if word not in self.vocab:\n",
        "            print(f\"{word} not in vocabulary!\")\n",
        "            return None\n",
        "\n",
        "        word_vector = self.get_word_vector(word)\n",
        "        similarities = {}\n",
        "\n",
        "        # Calculate similarity between the input word and all other words\n",
        "        for other_word in self.vocab:\n",
        "            if other_word != word:\n",
        "                other_vector = self.get_word_vector(other_word)\n",
        "                similarity = self.cosine_similarity(word_vector, other_vector)\n",
        "                similarities[other_word] = similarity\n",
        "\n",
        "        # Sort by similarity and return the top N most similar words\n",
        "        sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
        "        return sorted_similarities[:top_n]\n",
        "    def cosine_similarity(self, vec1, vec2):\n",
        "        \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
        "        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
      ],
      "metadata": {
        "id": "apiZg8qbeO7_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deepwalk"
      ],
      "metadata": {
        "id": "X6fdxfd3v9t_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomWalker:\n",
        "  def __init__(self, G, num_walks, walk_length):\n",
        "      \"\"\"\n",
        "      :param G: Graph\n",
        "      :param num_walks: a number of walks\n",
        "      :param walk_length: Length of a walk. Each walk is considered as a sentence\n",
        "      \"\"\"\n",
        "      self.G = G\n",
        "      self.num_walks = num_walks\n",
        "      self.walk_length = walk_length\n",
        "\n",
        "\n",
        "  def deepwalk_walk(self, start_node):\n",
        "      \"\"\"\n",
        "      :param start_node: Starting node of a walk\n",
        "      \"\"\"\n",
        "      walk = [start_node]\n",
        "      while len(walk) < self.walk_length:\n",
        "          cur = walk[-1]\n",
        "          # Check if having any neighbors at the current node\n",
        "          cur_nbrs = list(self.G.neighbors(cur))\n",
        "          if len(cur_nbrs) > 0:\n",
        "              # Random walk with the probability of 1/d(v^t). d(v^t) is the node degree\n",
        "              walk.append(random.choice(cur_nbrs))\n",
        "          else:\n",
        "              break\n",
        "      return walk\n",
        "\n",
        "\n",
        "  def simulate_walks(self, workers=1, verbose=0):\n",
        "      \"\"\"\n",
        "      :param workers: a number of workers running in parallel processing\n",
        "      :param verbose: progress bar\n",
        "      \"\"\"\n",
        "      G = self.G\n",
        "      nodes = list(G.nodes())\n",
        "      results = Parallel(n_jobs=workers, verbose=verbose)(\n",
        "          delayed(self._simulate_walks)(nodes) for num in\n",
        "          partition_num(self.num_walks, workers))\n",
        "      walks = list(itertools.chain(*results))\n",
        "      return walks\n",
        "\n",
        "\n",
        "  # INFORMATION EXTRACTOR\n",
        "  def _simulate_walks(self, nodes):\n",
        "      walks = []\n",
        "      # Iterate all walks per vertex\n",
        "      for _ in range(self.num_walks):\n",
        "          random.shuffle(nodes)\n",
        "          # Iterate all nodes in a walk\n",
        "          for v in nodes:\n",
        "            walks.append(self.deepwalk_walk(start_node=v))\n",
        "      return walks"
      ],
      "metadata": {
        "id": "76thQivwv_ca"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepWalk:\n",
        "    def __init__(self, model, graph, walk_length, num_walks, workers=1):\n",
        "\n",
        "        self.graph = graph\n",
        "        self.w2v_model = model\n",
        "        self._embeddings = {}\n",
        "\n",
        "        self.walker = RandomWalker(graph, num_walks=num_walks, walk_length=walk_length)\n",
        "        self.walks = self.walker.simulate_walks(workers=workers, verbose=1)\n",
        "        self.sentences = get_attributes_of_node(self.walks)\n",
        "\n",
        "\n",
        "    def train(self, window_size=5, epochs=100):\n",
        "        print(\"Learning embedding vectors...\")\n",
        "        training_data = preprocessing(self.sentences)\n",
        "        w2v = word2vec(window_size, epochs)\n",
        "        prepare_data_for_training(training_data, w2v)\n",
        "        w2v.train()\n",
        "        print(\"Learning embedding vectors done!\")\n",
        "        self.w2v_model = w2v\n",
        "\n",
        "\n",
        "    def test(self, word):\n",
        "        print(self.w2v_model.predict(word,3))"
      ],
      "metadata": {
        "id": "9ifUJ92X4QKX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run graph"
      ],
      "metadata": {
        "id": "3F0yyS6v4GMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "G = nx.read_edgelist('lab3_edgelist.txt',create_using=nx.DiGraph(),nodetype=None,data=[('weight',int)])# Read graph\n",
        "model = DeepWalk(word2vec(), G, walk_length=3, num_walks=10, workers=1)\n",
        "model.train(window_size=5)# train model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QL-fRxFN4H4j",
        "outputId": "80ee006a-2ba1-4c30-d370-0b5c0ec0b34e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning embedding vectors...\n",
            "Epoch 0, Loss: 8166.666899505466\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-6b8e520fe445>:85: RuntimeWarning: divide by zero encountered in log\n",
            "  loss += -np.sum(y_true * np.log(y_pred) )\n",
            "<ipython-input-10-6b8e520fe445>:85: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss += -np.sum(y_true * np.log(y_pred) )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100, Loss: nan\n",
            "Epoch 200, Loss: nan\n",
            "Learning embedding vectors done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.sentences)\n",
        "model.test(\"to\")\n",
        "model.test(\"this\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKzDQforUzXV",
        "outputId": "fe4b1c90-8d6a-4596-848f-dc45a9dbc0f2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['something', 'you', 'you'], ['learned', 'I', 'to'], ['am', 'this', 'I'], ['this', 'happy', 'that'], ['in', 'all', 'this'], ['the', 'all', 'am'], ['to', 'happy', 'wish'], ['I', 'you', 'you'], ['you', 'this', 'I'], ['so', '!', 'am'], ['this', 'to', '!'], ['lab', 'happy', 'wish'], ['lab', '.', 'so'], ['happy', 'that', 'this'], ['.', 'so', 'I'], ['you', 'to', 'happy'], ['that', 'this', 'so'], ['all', 'so', '!'], ['I', 'this', 'to'], ['!', 'am', 'am'], ['wish', 'learned', 'I'], ['.', '.', 'so'], ['best', 'I', 'to'], ['.', 'so', 'that'], ['the', 'I', 'this'], ['you', '.', 'so'], ['in', 'all', 'to'], ['happy', 'wish', 'that'], ['something', 'happy', 'that'], ['am', 'this', 'I'], ['this', 'this', 'happy'], ['wish', 'all', 'this'], ['so', 'that', 'this'], ['I', 'to', 'lab'], ['that', 'you', 'you'], ['lab', 'happy', 'wish'], ['lab', '.', 'so'], ['learned', 'this', 'this'], ['this', 'so', 'lab'], ['you', 'you', 'you'], ['to', 'lab', 'happy'], ['I', 'to', 'happy'], ['best', 'to', 'lab'], ['all', 'you', 'this'], ['.', '.', 'happy'], ['!', 'I', 'happy'], ['that', 'this', 'this'], ['.', 'happy', 'that'], ['you', 'you', 'you'], ['this', 'this', 'this'], ['best', 'to', 'lab'], ['am', 'am', 'this'], ['wish', 'learned', 'I'], ['so', 'that', 'this'], ['lab', 'am', 'you'], ['I', 'to', 'you'], ['!', 'am', 'am'], ['to', '!', 'I'], ['in', 'am', 'this'], ['something', 'you', 'you'], ['.', '.', 'happy'], ['this', 'this', 'so'], ['learned', '!', 'to'], ['all', 'to', 'lab'], ['lab', '.', 'happy'], ['the', 'so', 'lab'], ['I', 'happy', 'that'], ['you', '.', 'happy'], ['happy', 'that', 'this'], ['wish', 'learned', 'I'], ['best', 'to', '!'], ['I', 'happy', 'that'], ['you', '.', 'happy'], ['that', 'this', 'I'], ['to', '!', 'I'], ['this', 'happy', 'wish'], ['.', '.', '.'], ['.', 'happy', 'that'], ['learned', 'I', 'you'], ['this', 'this', 'to'], ['you', 'to', '!'], ['lab', '.', 'happy'], ['am', 'am', 'am'], ['I', 'to', '!'], ['!', 'so', '!'], ['happy', 'wish', 'learned'], ['something', 'wish', 'that'], ['in', 'to', 'lab'], ['the', 'all', 'this'], ['lab', 'am', 'you'], ['all', 'so', '!'], ['so', 'I', 'to'], ['this', 'I', 'happy'], ['something', 'you', 'you'], ['lab', '.', 'happy'], ['this', 'happy', 'that'], ['.', '.', 'happy'], ['the', 'so', 'lab'], ['lab', 'happy', 'wish'], ['.', 'happy', 'wish'], ['to', 'you', 'you'], ['I', 'happy', 'that'], ['best', 'I', 'happy'], ['so', 'I', 'am'], ['learned', 'this', 'to'], ['wish', 'learned', '!'], ['you', 'to', 'happy'], ['happy', 'wish', 'that'], ['you', '.', 'happy'], ['am', 'am', 'this'], ['I', 'happy', 'that'], ['that', 'you', 'you'], ['all', 'all', 'to'], ['!', 'am', 'this'], ['in', 'to', '!'], ['this', 'this', 'so'], ['.', '.', 'so'], ['learned', 'this', 'this'], ['something', 'learned', '!'], ['.', 'happy', 'that'], ['wish', 'learned', 'I'], ['so', 'lab', 'happy'], ['you', 'to', 'happy'], ['in', 'all', 'all'], ['!', 'I', 'this'], ['to', 'you', 'to'], ['I', 'to', 'lab'], ['the', 'all', 'you'], ['lab', 'am', 'you'], ['am', 'that', 'this'], ['lab', '.', 'so'], ['happy', 'that', 'you'], ['all', 'you', '.'], ['best', 'best', 'best'], ['you', 'I', 'to'], ['that', 'this', 'this'], ['this', 'I', 'to'], ['I', 'am', 'you'], ['to', '!', 'to'], ['I', 'this', 'to'], ['happy', 'wish', 'you'], ['lab', 'happy', 'wish'], ['learned', 'this', 'I'], ['all', 'this', 'so'], ['.', '.', 'happy'], ['this', 'this', 'happy'], ['something', 'you', 'to'], ['!', 'I', 'happy'], ['you', 'this', 'I'], ['wish', 'you', 'you'], ['in', 'am', 'that'], ['this', 'I', 'to'], ['lab', '.', 'so'], ['I', 'to', 'lab'], ['the', 'all', 'to'], ['.', 'so', '!'], ['you', 'to', 'lab'], ['best', 'to', '!'], ['that', 'this', 'this'], ['am', 'you', 'you'], ['so', 'I', 'you'], ['to', 'happy', 'wish'], ['the', 'so', '!'], ['so', 'I', 'happy'], ['.', '.', 'happy'], ['best', 'I', 'happy'], ['happy', 'that', 'you'], ['all', 'so', 'that'], ['I', 'happy', 'wish'], ['lab', '.', 'happy'], ['I', 'this', 'this'], ['lab', 'am', 'this'], ['this', 'I', 'you'], ['something', 'wish', 'you'], ['am', 'you', 'to'], ['you', 'I', 'am'], ['this', 'this', 'this'], ['wish', 'you', 'to'], ['in', 'am', 'am'], ['that', 'this', 'so'], ['learned', 'I', 'to'], ['!', 'I', 'to'], ['you', 'to', 'happy'], ['.', 'happy', 'wish'], ['wish', 'learned', 'I'], ['this', 'I', 'you'], ['something', 'happy', 'that'], ['so', '!', 'so'], ['!', 'am', 'that'], ['in', 'all', 'am'], ['you', 'to', 'lab'], ['.', 'so', 'lab'], ['this', 'happy', 'wish'], ['I', 'to', 'lab'], ['learned', '!', 'am'], ['.', '.', '.'], ['lab', '.', 'so'], ['am', 'this', 'to'], ['to', 'happy', 'wish'], ['lab', 'happy', 'wish'], ['that', 'this', 'to'], ['you', 'that', 'this'], ['the', 'so', 'lab'], ['happy', 'that', 'this'], ['all', 'am', 'this'], ['best', 'to', 'you'], ['I', 'this', 'to'], ['the', 'all', 'so'], ['.', 'so', 'that'], ['you', 'you', 'you'], ['this', 'happy', 'wish'], ['lab', 'happy', 'that'], ['something', 'you', 'you'], ['I', 'this', 'to'], ['wish', 'that', 'this'], ['lab', '.', 'so'], ['best', 'I', 'you'], ['all', 'you', 'that'], ['I', 'happy', 'wish'], ['happy', 'wish', 'you'], ['to', '!', 'I'], ['so', 'lab', 'am'], ['this', 'this', 'so'], ['learned', 'this', 'I'], ['am', 'you', 'you'], ['in', 'am', 'this'], ['!', 'I', 'this'], ['.', '.', '.'], ['you', 'I', 'happy'], ['that', 'you', 'you']]\n",
            "[('I', 0.9998257140933877), ('this', 0.9998170219643839), ('am', 0.9997113699059834)]\n",
            "[('I', 0.9998388061258124), ('to', 0.9998170219643839), ('am', 0.9997814110820127)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "z32hrgmu1Tpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "edges_path = 'facebook_edges.csv'\n",
        "targets_path = 'facebook_target.csv'\n",
        "features_path = 'facebook_features.json'"
      ],
      "metadata": {
        "id": "vmYz-Qyf1U4Y"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Load edges\n",
        "edges_df = pd.read_csv(edges_path)\n",
        "\n",
        "# Load features\n",
        "with open(features_path, 'r') as f:\n",
        "    features = json.load(f)\n",
        "\n",
        "# Create the graph\n",
        "G = nx.from_pandas_edgelist(edges_df, 'id_1', 'id_2', create_using=nx.Graph())\n",
        "\n",
        "nx.set_node_attributes(G, features, 'features')\n",
        "\n",
        "# Initialize and train DeepWalk\n",
        "# Define hyperparameters for DeepWalk\n",
        "walk_length = 10\n",
        "num_walks = 80\n",
        "window_size = 5\n",
        "workers = 2\n"
      ],
      "metadata": {
        "id": "GE7CqYeSuMnM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train DeepWalk\n",
        "model = DeepWalk(word2vec(), G, walk_length=walk_length, num_walks=num_walks, workers=workers)\n",
        "model.train(window_size=window_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "OUTl2VEll9WV",
        "outputId": "87b8dac0-81b3-4282-c402-8d2b3db4bc73"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:  1.6min finished\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "14638",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-0fb5baafa99e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train DeepWalk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeepWalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwalk_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwalk_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_walks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_walks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-8bd71ccfedbe>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, graph, walk_length, num_walks, workers)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomWalker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_walks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_walks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwalk_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwalk_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulate_walks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_attributes_of_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-1ce8db2e8fc3>\u001b[0m in \u001b[0;36mget_attributes_of_node\u001b[0;34m(node_paths)\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m       \u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_attr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mnode_paths_attributes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnode_paths_attributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 14638"
          ]
        }
      ]
    }
  ]
}